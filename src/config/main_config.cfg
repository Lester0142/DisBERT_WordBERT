[args]
# ========================================
# Model and Data Configuration
# ========================================
# Bert-based pre-trained model ('bert-base-cased', 'roberta-base', 'albert-base-v1')
bert_model = roberta-base
# Data directory that contains .tsv files ('VUA18' / 'MOH-X/CLS' / 'TroFi/CLS' / 'VUA20')
data_dir = data/VUA18
# Training methodoloy ('vua'(1-fold) / 'trofi'(10-fold))
task_name = vua
# The name of model type ('DisBERT' / 'WordBERT' / 'DefBERT')
model_name = DisBERT
# If model is cased set to 'True' ('True' / 'False')
do_lower_case = False

# ========================================
# Run Configuration
# ========================================
#  Option to run training 
do_train = True
# Option to run testing
do_test = True
# Option to run evaluation
do_eval = True
# CUDA or CPU
no_cuda = False
# Random seed
seed = 42

# ========================================
# Hyperparameters
# ========================================
# The hidden layer size of classifier
classifier_hidden = 768
# Learning rate scheduler ('none' / 'warmup_linear')
lr_scheduler = warmup_linear
# Number of warm-up epoch for scheduler
warmup_epoch = 2
# Dropout ratio 
dropout_rate = 0.2
# Use POS tag as training feature
use_pos = True
# Tag local context with respect to target word
use_local_context= True
# The maximum total input sequence length after WordPiece tokenization. (default = 200)
max_seq_length = 150
# Weight of Metaphor class
class_weight = 3
# Batch size for training
train_batch_size = 32
# Batch Size for evaluation
eval_batch_size = 8
# Learning rate for AdamW optimizer
learning_rate = 3e-5
# Number of training epochs
train_epoch = 3