[args]
# ========================================
# Model and Data Configuration
# ========================================
# Bert-based pre-trained model ('bert-base-cased', 'roberta-base', 'albert-base-v1')
bert_model = roberta-base
# Data directory that contains .tsv files ('VUA18' / 'MOH-X/CLS' / 'TroFi/CLS' / 'VUA20')
data_dir = data/TroFi/CLS
# Training methodoloy ('vua'(1-fold) / 'trofi'(10-fold))
task_name = trofi
# The name of model type ('DisBERT' / 'WordBERT' / 'DefBERT')
model_name = DisBERT
# If model is cased set to 'True' else 'False' ('True' / 'False')
do_lower_case = False

# ========================================
# Run Configuration
# ========================================
#  Option to run training ('True' / 'False')
do_train = True
# Option to run testing ('True' / 'False')
do_test = True
# Option to run evaluation ('True' / 'False')
do_eval = True
# CUDA or CPU ('True' / 'False')
no_cuda = False
# Random seed (int)
seed = 42

# ========================================
# Hyperparameters
# ========================================
# The hidden layer size of classifier (int)
classifier_hidden = 768
# Learning rate scheduler ('none' / 'warmup_linear')
lr_scheduler = warmup_linear
# Number of warm-up epoch for scheduler (int)
warmup_epoch = 2
# Dropout ratio (float)
dropout_rate = 0.2
# Use POS tag as training feature ('True' / 'False')
use_pos = True
# Tag local context with respect to target word ('True' / 'False')
use_local_context= True
# The maximum total input sequence length after WordPiece tokenization (int)
max_seq_length = 150
# Weight of Metaphor class (int)
class_weight = 3
# Batch size for training (int)
train_batch_size = 32
# Batch Size for evaluation (int)
eval_batch_size = 8
# Learning rate for AdamW optimizer (float)
learning_rate = 3e-5
# Number of training epochs (int)
train_epoch = 3